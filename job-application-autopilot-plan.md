# ğŸš€ Job Application Autopilot â€” Project Plan

> An AI-powered agent that searches Indian job platforms based on your resume, runs ATS analysis,
> tailors your resume with user confirmation at every major step, benchmarks against open-source
> reference resumes, writes cover letters, and integrates with Claude Code & Google Antigravity via MCP.

---

## ğŸ¯ What It Does

Load your resume â†’ the agent analyses your skills and experience and uses that to drive job searches across Indian platforms (Naukri, LinkedIn, Indeed, CutShort, and more). For each match, it runs a full ATS check, benchmarks your resume against open-source reference resumes for that role, and proposes tailored changes. **You review every change before anything is saved.** When applying, the agent shows you the full job details and asks for your explicit approval first. Nothing is submitted without you saying yes.

No credentials are ever stored. Platform sessions use a one-time browser login (Playwright), after which a session cookie is saved locally â€” your password never touches the tool.

---

## âœ¨ Core Features

| Feature | Description |
|---|---|
| ğŸ“„ Resume Parser | Parses PDF/DOCX via **Docling** â€” extracts text, structure, reading order, headers/footers |
| ğŸ¯ Resume-Based Search | Analyses your resume first, then searches for roles matching your actual skills and experience |
| ğŸŒ Platform Search | Searches Naukri, LinkedIn, Indeed, CutShort and more for matching roles |
| ğŸ” Job Fetcher | Scrapes full job descriptions from any URL or pasted text |
| ğŸ¤– ATS Checker | Scores your resume against 3 ATS layers: formatting, keyword match, and data integrity |
| ğŸ“š Reference Resumes | Benchmarks your resume against open-source high-quality resumes for the same role |
| ğŸ§  Resume Tailor | Proposes changes with a diff view â€” **you approve before anything is saved** |
| ğŸ›¡ï¸ Apply Gate | Shows full job details and asks for explicit confirmation before submitting any application |
| âœ‰ï¸ Cover Letter Writer | Generates a personalised, professional cover letter |
| ğŸ“Š Application Tracker | SQLite-powered dashboard to track every application, status, and notes |
| ğŸ”Œ MCP Server | Exposes all tools so Claude Code & Antigravity can use them conversationally |
| ğŸ’» CLI | Standalone Typer-based CLI for users without an AI assistant |

---

## ğŸ—ï¸ Architecture

```
User / AI Assistant (Claude Code / Antigravity)
            â”‚
            â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   MCP Server     â”‚  â† FastMCP (same pattern as your laptop-assistant)
    â”‚  (server.py)     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                   Agent Layer                      â”‚
    â”‚  ResumeParserAgent   JobFetcherAgent               â”‚
    â”‚  ATSCheckerAgent     TailorAgent                   â”‚
    â”‚  CoverLetterAgent    TrackerAgent                  â”‚
    â”‚  PlatformAgent                                     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Docling              â”‚   â”‚  LLM (Structured Output)   â”‚
    â”‚  Document Engine      â”‚   â”‚  Keyword / skill extract   â”‚
    â”‚  PDF/DOCX â†’ JSON      â”‚   â”‚  Inferred skill detection  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚  Acronym expansion         â”‚
             â”‚                  â”‚  Experience / edu parsing  â”‚
             â”‚                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚                            â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  ATS Engine                          â”‚   â”‚  Storage Layer        â”‚
    â”‚  Layer 1: Formatting  (Docling JSON) â”‚   â”‚  SQLite DB            â”‚
    â”‚  Layer 2: Keywords    (LLM JSON)     â”‚   â”‚  Resume Store         â”‚
    â”‚  Layer 3: Integrity   (Regex+Python) â”‚   â”‚  Output Files         â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸŒ Supported Job Platforms

### General & Professional
| Platform | Access Method | What We Use It For |
|---|---|---|
| **Naukri.com** | Playwright session (browser login once) | Search, fetch JDs, track applied |
| **LinkedIn** | Official OAuth API + Playwright fallback | Search, fetch JDs, Easy Apply |
| **Indeed India** | Public search API + scraping | Aggregate job search |
| **Foundit** (Monster) | Playwright session | White/blue collar roles |
| **Shine.com** | Playwright session | Entry to mid-level roles |

### Startup & Tech Focused
| Platform | Access Method | What We Use It For |
|---|---|---|
| **CutShort** | Public API | AI-matched startup roles |
| **Instahyre** | Playwright session | High-quality tech roles |
| **Wellfound** | Public API | Startup ecosystem |

### How Login Works (No Password Storage)
```
First run:
  Agent: "Do you have a Naukri account? I'll open a browser window â€”
          you log in once, and I'll save the session for future searches."
  â†’ Playwright opens a real browser window
  â†’ User logs in manually (password never touches the tool)
  â†’ Session cookie saved to storage/sessions/naukri.json
  â†’ All future searches reuse the cookie silently

Subsequent runs:
  â†’ Tool loads saved cookie and searches directly
  â†’ If session expires, prompts for a fresh manual login
```

---

## ğŸ¤– ATS Checker Engine

The ATS Checker simulates how real corporate hiring software (Workday, Taleo, Greenhouse) parses and scores resumes. It runs in 3 layers.

### Layer 1 â€” Parsing & Formatting ("Can I Read It?")
Docling first converts the resume to a structured JSON. The formatter then checks:

| Rule | What We Check | Flag Severity |
|---|---|---|
| **File Type** | Only `.pdf` and `.docx` accepted | âŒ Hard block |
| **Multi-column Layout** | Detects columns that cause text jumbling | âš ï¸ Warning |
| **Text Boxes / Frames** | Often skipped by older parsers | âš ï¸ Warning |
| **Section Headers** | Must use standard names: *Experience*, *Education*, *Skills* | âš ï¸ Warning |
| **Contact in Header/Footer** | Docling detects headers/footers; email/phone must be in body | âš ï¸ Warning |
| **Non-standard Bullets** | Arrows, ticks, custom chars â†’ flag for standard `â€¢` or `-` | â„¹ï¸ Info |
| **Obscure Fonts** | Fonts that corrupt on extraction | â„¹ï¸ Info |

### Layer 2 â€” Keyword & Content ("Is It Relevant?")
The LLM first parses both the job description and the resume into structured JSON using **JSON mode / structured output**. No traditional NLP library needed.

**Step 1 â€” JD Extraction (LLM structured output):**
```json
{
  "required_hard_skills": ["Python", "Kafka", "Kubernetes", "gRPC"],
  "preferred_hard_skills": ["FastAPI", "Redis", "PostgreSQL"],
  "soft_skills": ["leadership", "communication"],
  "experience_required_years": 3,
  "education_required": "Bachelor's",
  "acronyms": {"AWS": "Amazon Web Services", "K8s": "Kubernetes"}
}
```

**Step 2 â€” Resume Skill Extraction (LLM structured output):**
```json
{
  "hard_skills": ["Python", "FastAPI", "Docker", "PostgreSQL", "Redis"],
  "inferred_skills": ["React (inferred from: led the frontend rewrite in React)"],
  "soft_skills": ["team lead", "mentoring"],
  "job_titles": ["Senior Software Engineer", "Backend Developer"],
  "total_experience_years": 4.5
}
```

**Step 3 â€” Scoring (pure Python logic, no library):**

| Rule | Logic |
|---|---|
| **Exact Keyword Match** | Hard skills matched exactly between JD extract and resume extract |
| **Contextual / Inferred Skills** | LLM flags skills inferred from bullet text (e.g., "led React rewrite" â†’ React) |
| **Acronym Expansion** | LLM extracts both forms; checker advises including both in resume |
| **Keyword Density** | Count occurrences in resume text. Optimal: 2â€“3Ã—. Flag if >5% of word count |
| **Contextual Placement Score** | Keyword in Job Title = 5pts, in Experience bullets = 3pts, in Skills list = 1pt |
| **Match Percentage** | `(matched_keywords / total_jd_keywords) Ã— 100` |
| **Missing Keywords List** | Explicit list of JD keywords absent from resume extract |

### Layer 3 â€” Data Integrity ("Is It Complete?")
Contact info and date validation use **regex only** (zero dependencies). Employment gaps,
experience totals, and education level come from the LLM structured extract in Layer 2.

| Rule | Method | What We Check |
|---|---|---|
| **Contact Info â€” Email** | Regex | `[\w.-]+@[\w.-]+\.\w+` present in body (not header/footer per Docling) |
| **Contact Info â€” Phone** | Regex | Indian (`+91`, 10-digit) and international formats |
| **Date Formats** | Regex | Validates `MM/YYYY` or `Month YYYY`; flags fuzzy dates like "Winter 2023" or bare "2023" |
| **Employment Gaps** | LLM extract + Python | Calculates gaps from structured date list; flags gaps > 6 months |
| **Years of Experience** | LLM extract + Python | Sums durations from structured JSON; compares against JD requirement |
| **Education Level** | LLM extract | Degree tier (Bachelor's / Master's / PhD) identified contextually; checked against JD |

### ATS Score Output Example
```
ATS Report â€” Razorpay Senior Python Developer
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Overall Score:       73 / 100   âš ï¸  Needs Improvement

Layer 1 â€” Formatting:   18 / 20
  âœ… File type: PDF (compatible)
  âœ… No multi-column layout detected
  âš ï¸  Section header "Career History" â†’ rename to "Experience"
  âš ï¸  Phone number found in footer â†’ move to body

Layer 2 â€” Keywords:     38 / 60
  Match: 63% (19/30 JD keywords found)
  âœ… Found: Python, FastAPI, PostgreSQL, Redis, Docker, CI/CD
  âŒ Missing: Kafka, gRPC, Kubernetes, payments domain, high-throughput
  âš ï¸  "AWS" found but not "Amazon Web Services" â€” include both
  âš ï¸  "Python" mentioned 7Ã— â€” slightly high, aim for 2â€“3Ã—

Layer 3 â€” Integrity:    17 / 20
  âœ… Email and phone present
  âœ… All dates in MM/YYYY format
  âœ… 4.5 years experience detected (JD requires 3+)
  âš ï¸  6-month gap (Jun 2022â€“Dec 2022) â€” consider adding context

Recommendation: Tailor resume to fix the above before applying.
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

---

## ğŸ“š Reference Resume Library

When tailoring a resume, the tool benchmarks against open-source, high-quality reference resumes for the same role. This gives the LLM a concrete target to optimise toward â€” not just "make it better" but "make it closer to what a strong candidate for this role actually looks like."

### How It Works

```
1. User loads resume + fetches job (e.g., "Senior Python Developer")
2. Tool queries reference library for matching role + seniority level
3. Reference resume is parsed via Docling â†’ structured JSON
4. TailorAgent receives: user_resume + job_description + reference_resume
5. LLM uses reference as a benchmark:
     - What sections does the reference have that yours is missing?
     - What phrasing patterns does the reference use for this role?
     - What skills are prominently featured that you also have but buried?
6. All suggestions are shown as a diff â€” you decide what to keep
```

### Reference Resume Sources

| Source | What's Available | How We Use It |
|---|---|---|
| [resume-dataset (GitHub)](https://github.com/florex/resume_corpus) | 2,000+ anonymised resumes across roles | Seed the local library |
| [Open Resume](https://github.com/xitanggg/open-resume) | Clean, ATS-optimised resume templates | Use as structural reference |
| [Awesome CV](https://github.com/posquit0/Awesome-CV) | LaTeX-based high-quality templates | Use as formatting benchmark |
| Community contributions | Users can contribute anonymised resumes | Crowdsourced quality over time |

### Storage
Reference resumes are stored locally in `reference_resumes/<role>/` as pre-parsed JSON â€” no network call at tailoring time, fully offline.

```
reference_resumes/
â”œâ”€â”€ software_engineer/
â”‚   â”œâ”€â”€ senior_python_dev_ref1.json
â”‚   â”œâ”€â”€ senior_python_dev_ref2.json
â”‚   â””â”€â”€ backend_engineer_ref1.json
â”œâ”€â”€ data_scientist/
â”‚   â””â”€â”€ data_scientist_ref1.json
â”œâ”€â”€ product_manager/
â”‚   â””â”€â”€ ...
â””â”€â”€ index.json    â† role â†’ file mapping for fast lookup
```

---

## ğŸ›¡ï¸ User Confirmation & Safety Gates

The tool **never makes irreversible changes silently.** Every significant action requires explicit user approval. There are three confirmation gates:

### Gate 1 â€” Resume Change Preview (before saving any tailored version)

Triggered when the tailor agent proposes changes to more than 2 bullet points, or makes structural changes (new sections, renamed headers). Shows a clear diff:

```
Resume Changes Proposed â€” Razorpay Senior Python Developer
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

  SECTION HEADER
  - "Career History"
  + "Experience"

  BULLET â€” Software Engineer @ Acme Corp
  - "Worked on backend services using Python and handled APIs"
  + "Built high-throughput REST APIs in Python (FastAPI) processing
     2M+ daily requests, with Redis caching and PostgreSQL"

  BULLET â€” Software Engineer @ Acme Corp
  - "Used Docker for deployments"
  + "Containerised microservices with Docker and orchestrated with
     Kubernetes across 3 environments (dev/staging/prod)"

  NEW BULLET ADDED â€” Skills Section
  + "Kafka Â· gRPC Â· Payments domain Â· Amazon Web Services (AWS)"

  Reference benchmark: Senior Python Dev @ Fintech (open-source)
  ATS Score: 73 â†’ 91 (+18 points)

  Apply these changes? [Yes / No / Edit manually]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

### Gate 2 â€” Job Application Confirmation (before submitting to any platform)

Triggered before clicking "Apply" on any platform. Shows the full job card:

```
Confirm Application â€” Razorpay
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
  Role:       Senior Python Developer
  Company:    Razorpay
  Location:   Bangalore (Hybrid)
  Salary:     â‚¹18â€“25 LPA
  Platform:   LinkedIn Easy Apply
  Posted:     2 days ago
  JD Summary: Payments infra team, Python/FastAPI, Kafka, K8s, 3+ yrs

  Resume:     outputs/razorpay-resume.docx  (ATS Score: 91/100)
  Cover:      outputs/razorpay-cover-letter.docx

  Ready to apply? [Yes / No / Preview resume first]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

### Gate 3 â€” Minor Tweaks (no confirmation needed)

Small changes (rewording a single bullet, adding an acronym expansion, fixing a section header) are applied silently and logged. The user can always run `show-changes --job-id 3` to review the full change history.

---

## ğŸ› ï¸ Tech Stack

| Layer | Tool | Reason |
|---|---|---|
| Language | Python 3.10+ | Consistency with your other projects |
| LLM (MCP mode) | **None required** | Claude Code uses Claude; Antigravity uses Gemini â€” the host AI handles all language tasks |
| LLM (CLI mode) | Anthropic / Google / Ollama | Powers keyword extraction (structured JSON), NER, and all language tasks in CLI mode |
| **Document Parsing** | **`docling`** (IBM Research) | Replaces pdfplumber + python-docx; handles PDF/DOCX with layout, reading order, header/footer detection |
| **Keyword Extraction** | **LLM Structured Output** | Replaces spaCy â€” LLM extracts skills, acronyms, experience, and context-aware entities via JSON mode |
| **Contact / Date Validation** | **Regex** | Simple, zero-dependency pattern matching for email, phone, and date formats |
| Job Scraping | `httpx` + `BeautifulSoup4` | Lightweight, already in your mcp_servers stack |
| Browser Automation | `Playwright` | Secure session-based login to job platforms |
| Database | SQLite | Same pattern as AI-Meme-Lab's memory store |
| MCP Server | `FastMCP` | Already mastered in mcp_servers repo |
| CLI | `Typer` | Same as AI-Meme-Lab |
| Output Docs | `python-docx` | Export tailored resume and cover letter as DOCX |

### Why Docling over pdfplumber + python-docx?

| Capability | pdfplumber + python-docx | Docling |
|---|---|---|
| PDF text extraction | âœ… | âœ… |
| DOCX extraction | âœ… | âœ… |
| Reading order detection | âŒ | âœ… |
| Header/footer detection | âŒ | âœ… â† critical for ATS |
| Table structure extraction | Partial | âœ… |
| Multi-column layout detection | âŒ | âœ… â† critical for ATS |
| Output as structured JSON | âŒ | âœ… |
| LangChain / LlamaIndex ready | âŒ | âœ… |
| Scanned PDF (OCR) | âŒ | âœ… |

---

## ğŸ“ Project Structure

```
job-application-autopilot/
â”œâ”€â”€ server.py                     # MCP server entry point
â”œâ”€â”€ cli.py                        # Typer CLI entry point
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ .env.example
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”œâ”€â”€ resume_parser.py      # Docling-powered PDF/DOCX parser â†’ structured JSON
â”‚   â”‚   â”œâ”€â”€ resume_profiler.py    # Extracts skills/roles from resume to drive job search
â”‚   â”‚   â”œâ”€â”€ ats_checker.py        # 3-layer ATS analysis engine
â”‚   â”‚   â”œâ”€â”€ reference_agent.py    # Fetches & matches open-source reference resumes by role
â”‚   â”‚   â”œâ”€â”€ diff_viewer.py        # Generates before/after diff for user review
â”‚   â”‚   â”œâ”€â”€ job_fetcher.py        # Scrapes or parses job descriptions from URLs
â”‚   â”‚   â”œâ”€â”€ platform_agent.py     # Orchestrates search across job platforms
â”‚   â”‚   â”œâ”€â”€ tailor_agent.py       # LLM rewrites resume using ATS report + reference resume
â”‚   â”‚   â”œâ”€â”€ apply_agent.py        # Handles job application with confirmation gate
â”‚   â”‚   â”œâ”€â”€ cover_letter_agent.py # Generates cover letter
â”‚   â”‚   â””â”€â”€ tracker_agent.py      # Manages application records
â”‚   â”‚
â”‚   â”œâ”€â”€ ats/                      # ATS engine sub-modules
â”‚   â”‚   â”œâ”€â”€ formatter_check.py    # Layer 1: formatting & structure rules (Docling JSON)
â”‚   â”‚   â”œâ”€â”€ jd_extractor.py       # LLM structured output â†’ JD skills/requirements JSON
â”‚   â”‚   â”œâ”€â”€ resume_extractor.py   # LLM structured output â†’ resume skills/experience JSON
â”‚   â”‚   â”œâ”€â”€ keyword_scorer.py     # Layer 2: pure Python scoring logic (no NLP library)
â”‚   â”‚   â”œâ”€â”€ integrity_check.py    # Layer 3: regex for contact/dates + Python gap calc
â”‚   â”‚   â””â”€â”€ report.py             # Formats and renders the ATS score report
â”‚   â”‚
â”‚   â”œâ”€â”€ platforms/                # One file per job platform
â”‚   â”‚   â”œâ”€â”€ base.py               # Abstract base class for all platforms
â”‚   â”‚   â”œâ”€â”€ naukri.py             # Naukri.com Playwright integration
â”‚   â”‚   â”œâ”€â”€ linkedin.py           # LinkedIn OAuth + API integration
â”‚   â”‚   â”œâ”€â”€ indeed.py             # Indeed India scraper
â”‚   â”‚   â”œâ”€â”€ cutshort.py           # CutShort API integration
â”‚   â”‚   â”œâ”€â”€ foundit.py            # Foundit Playwright integration
â”‚   â”‚   â””â”€â”€ wellfound.py          # Wellfound API integration
â”‚   â”‚
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ llm.py                # OpenAI / Ollama client (dual provider)
â”‚   â”‚   â”œâ”€â”€ docling_parser.py     # Wraps Docling DocumentConverter
â”‚   â”‚   â”œâ”€â”€ scraper.py            # Generic job URL scraper
â”‚   â”‚   â”œâ”€â”€ session_manager.py    # Playwright browser sessions & cookies
â”‚   â”‚   â””â”€â”€ doc_exporter.py       # Exports tailored resume + cover letter as DOCX
â”‚   â”‚
â”‚   â”œâ”€â”€ storage/
â”‚   â”‚   â”œâ”€â”€ database.py           # SQLite setup and queries
â”‚   â”‚   â”œâ”€â”€ schema.sql            # resumes, jobs, applications, ats_reports tables
â”‚   â”‚   â””â”€â”€ sessions/             # Saved browser session cookies (gitignored)
â”‚   â”‚
â”‚   â””â”€â”€ tools/                    # MCP tool definitions (one file per domain)
â”‚       â”œâ”€â”€ resume_tools.py
â”‚       â”œâ”€â”€ ats_tools.py          # check_ats, get_ats_report, get_missing_keywords
â”‚       â”œâ”€â”€ reference_tools.py    # get_reference_resumes, show_benchmark
â”‚       â”œâ”€â”€ diff_tools.py         # preview_changes, confirm_changes, show_history
â”‚       â”œâ”€â”€ job_tools.py
â”‚       â”œâ”€â”€ platform_tools.py
â”‚       â”œâ”€â”€ application_tools.py  # apply_job (with confirmation gate)
â”‚       â””â”€â”€ output_tools.py
â”‚
â”œâ”€â”€ reference_resumes/            # Open-source reference resumes (pre-parsed JSON)
â”‚   â”œâ”€â”€ software_engineer/
â”‚   â”œâ”€â”€ data_scientist/
â”‚   â”œâ”€â”€ product_manager/
â”‚   â”œâ”€â”€ devops_engineer/
â”‚   â””â”€â”€ index.json                # role â†’ files mapping
â”‚
â”œâ”€â”€ outputs/                      # Generated resumes, cover letters, ATS reports
â”œâ”€â”€ assets/
â”‚   â””â”€â”€ resume.pdf                # User drops their master resume here
â”‚
â””â”€â”€ tests/
    â”œâ”€â”€ test_parser.py
    â”œâ”€â”€ test_ats_checker.py
    â”œâ”€â”€ test_reference_agent.py
    â”œâ”€â”€ test_diff_viewer.py
    â”œâ”€â”€ test_tailor.py
    â”œâ”€â”€ test_platforms.py
    â””â”€â”€ test_tracker.py
```

---

## ğŸ”Œ MCP Tools (for Claude Code & Antigravity)

| Tool | Description | Example Prompt |
|---|---|---|
| `load_resume(file_path)` | Parse and store your master resume via Docling | *"Load my resume from Downloads/resume.pdf"* |
| `profile_resume()` | Extract skills/roles from resume to drive job search | *"What roles am I best suited for?"* |
| `connect_platform(name)` | Open browser for one-time login to a job platform | *"Connect my Naukri account"* |
| `list_platforms()` | Show which platforms are connected | *"Which job sites am I connected to?"* |
| `search_jobs(location, platforms?)` | Search jobs based on your resume profile | *"Find jobs for me in Bangalore"* |
| `fetch_job(url)` | Scrape and parse a single job posting | *"Fetch this job: linkedin.com/jobs/..."* |
| `check_ats(job_id)` | Run full 3-layer ATS analysis on your resume vs a job | *"ATS check my resume against job #3"* |
| `get_ats_report(job_id)` | Get the detailed ATS score report | *"Show me the full ATS report for Razorpay"* |
| `get_missing_keywords(job_id)` | Get list of keywords your resume is missing | *"What keywords am I missing for this role?"* |
| `get_reference_resumes(role)` | Fetch matching open-source reference resumes | *"Show me reference resumes for Python developer"* |
| `tailor_resume(job_id)` | Propose resume changes with diff preview â€” user confirms | *"Tailor my resume for job #3"* |
| `preview_changes(job_id)` | Show pending resume changes before saving | *"Show me what changes were proposed"* |
| `confirm_changes(job_id)` | Accept and save the tailored resume | *"Yes, apply those changes"* |
| `show_change_history(job_id)` | Review all changes made for a specific job | *"What did you change for the Razorpay resume?"* |
| `apply_job(job_id)` | Show job card and ask for confirmation before applying | *"Apply for job #3"* |
| `generate_cover_letter(job_id)` | Write a personalised cover letter | *"Write a cover letter for the Stripe role"* |
| `export_application(job_id)` | Export resume + cover letter as DOCX | *"Export my application for job #3"* |
| `add_application(company, role, url)` | Log a new application manually | *"I just applied to Google SWE"* |
| `update_status(app_id, status, notes)` | Update application status | *"Mark Stripe as 'Interview Scheduled'"* |
| `list_applications(status?)` | View tracked applications | *"Show me all pending applications"* |
| `get_stats()` | View application statistics | *"How many jobs have I applied to this month?"* |

---

## ğŸ’¬ Example Conversations (with Claude Code / Antigravity)

**First-time platform setup:**
```
You:    "Help me start my job search"
Agent:  "Do you have accounts on any job platforms? I support Naukri, LinkedIn,
         Indeed, CutShort, Wellfound, and Foundit.
         I'll open a browser window â€” you log in once, and I handle the rest."
You:    "I have Naukri and LinkedIn"
Agent:  "Opening Naukri in your browser now. Log in and come back here when done."
        â†’ [User logs in manually in browser window]
Agent:  "Naukri session saved! Opening LinkedIn now..."
        â†’ [User logs in manually]
Agent:  "Both platforms connected. What kind of role are you looking for?"
```

**Resume-driven search + reference benchmark:**
```
You:    "Find me jobs in Bangalore"
Agent:  "Analysing your resume... Detected profile:
         â†’ 4.5 yrs Python backend, FastAPI, PostgreSQL, Docker, some K8s
         â†’ Best-fit roles: Senior Python Developer, Backend Engineer,
           Platform Engineer (mid-to-senior)
         Searching Naukri and LinkedIn... Found 24 relevant jobs.
         Top match: Senior Python Dev at Razorpay (â‚¹18-25L) â€” 94% fit
         Want me to run an ATS check + benchmark against a reference resume?"
You:    "Yes"
Agent:  "ATS Score: 73/100 âš ï¸  Missing: Kafka, gRPC, Kubernetes, payments domain
         Reference benchmark (Senior Python Dev @ Fintech):
         â†’ Their experience bullets are 30% more achievement-oriented
         â†’ They lead with impact metrics; yours lead with technologies
         Want to see the proposed changes?"
```

**Resume diff + user confirmation:**
```
You:    "Yes, show me the changes"
Agent:  "Here's what I'd change:

         HEADER: 'Career History' â†’ 'Experience'

         BULLET (Acme Corp):
         - 'Worked on backend services using Python and handled APIs'
         + 'Built high-throughput REST APIs (FastAPI) processing 2M+ daily
            requests with Redis caching â€” reduced p99 latency by 40%'

         NEW SKILL: 'Kafka Â· gRPC Â· Amazon Web Services (AWS)'

         ATS Score after: 73 â†’ 91 (+18)
         Apply these changes? [Yes / No / Edit manually]"
You:    "Yes"
Agent:  "Saved. Generating cover letter now..."
```

**Apply gate â€” explicit confirmation before submitting:**
```
You:    "Apply for the Razorpay job"
Agent:  "Confirm Application:
         â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
         Role:     Senior Python Developer
         Company:  Razorpay
         Location: Bangalore (Hybrid)
         Salary:   â‚¹18â€“25 LPA
         Platform: LinkedIn Easy Apply
         Posted:   2 days ago
         Resume:   razorpay-resume.docx (ATS: 91/100)
         â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
         Shall I submit this application? [Yes / No]"
You:    "Yes"
Agent:  "Application submitted. Logged to tracker."
```

**Checking your pipeline:**
```
You:    "How's my job search going?"
Agent:  "You have 12 applications tracked:
         - 4 Pending response
         - 3 Interview scheduled
         - 2 Offer received
         - 3 Rejected
         Average ATS score before tailoring: 71. After tailoring: 89."
```

---

## ğŸ—“ï¸ Development Roadmap

### Phase 1 â€” Core Engine (Days 1â€“3)
- [ ] Set up project structure and `pyproject.toml`
- [ ] Integrate **Docling** as the primary document parser (`docling_parser.py`)
- [ ] Build `ResumeParserAgent` â€” Docling â†’ structured resume JSON
- [ ] Build `ResumeProfiler` â€” extract skills, experience level, and best-fit roles from resume
- [ ] Build `JobFetcherAgent` â€” scrape job posting or parse pasted text
- [ ] Build `CoverLetterAgent` â€” LLM drafts cover letter
- [ ] Set up `LLMService` with Anthropic/Google/Ollama support (no key needed in MCP mode)

### Phase 2 â€” ATS Checker (Days 4â€“6)
- [ ] `formatter_check.py` â€” Layer 1: file type, multi-column, headers/footers, section names, bullets (from Docling JSON)
- [ ] `jd_extractor.py` â€” LLM structured output prompt â†’ extracts required skills, acronyms, experience, education from JD
- [ ] `resume_extractor.py` â€” LLM structured output prompt â†’ extracts skills (including inferred), titles, dates from resume
- [ ] `keyword_scorer.py` â€” Layer 2: pure Python scoring logic using the two extracted JSON objects
- [ ] `integrity_check.py` â€” Layer 3: regex for email/phone/dates + Python gap/experience calculation
- [ ] `report.py` â€” formats scored output into the ATS report card
- [ ] `ATSCheckerAgent` â€” orchestrates all 3 layers and returns final score

### Phase 3 â€” Reference Resume Library (Days 7â€“8)
- [ ] Curate open-source reference resumes (resume_corpus, Open Resume, Awesome CV) for common roles
- [ ] Pre-parse all reference resumes via Docling â†’ store as JSON in `reference_resumes/`
- [ ] Build `reference_agent.py` â€” match user's role to relevant reference resumes by title + seniority
- [ ] Build `index.json` â€” role keyword â†’ reference file mapping for fast lookup
- [ ] Extend `TailorAgent` to receive reference resume as additional context

### Phase 4 â€” Tailor + Diff + Confirmation Gates (Days 9â€“10)
- [ ] `TailorAgent` â€” LLM receives resume JSON + ATS report + reference resume â†’ proposes changes
- [ ] `DiffViewer` â€” generates clean before/after diff for every proposed change
- [ ] **Gate 1**: show diff to user, require `confirm_changes()` before saving (>2 bullet changes)
- [ ] **Gate 3**: minor changes (1â€“2 bullets, acronym fixes) applied silently and logged
- [ ] Re-runs ATS check after tailoring to confirm score improvement
- [ ] `DocExporter` â€” outputs confirmed tailored resume as clean DOCX

### Phase 5 â€” Storage, Tracking & Apply Gate (Days 11â€“12)
- [ ] SQLite schema: `resumes`, `jobs`, `applications`, `ats_reports`, `change_history` tables
- [ ] `TrackerAgent` â€” CRUD for application records
- [ ] `ApplyAgent` â€” **Gate 2**: shows full job card, requires `confirm_apply()` before submitting
- [ ] Typer CLI: `load`, `profile`, `search`, `fetch`, `check-ats`, `benchmark`, `tailor`, `preview`, `confirm`, `apply`, `track`, `stats`

### Phase 5 â€” Job Platform Integration (Days 9â€“11)
- [ ] `SessionManager` â€” Playwright browser launch, cookie save/load, session expiry detection
- [ ] `NaukriPlatform` â€” search jobs, fetch JDs via saved session
- [ ] `LinkedInPlatform` â€” OAuth setup + job search API
- [ ] `IndeedPlatform` â€” public search scraper
- [ ] `CutShortPlatform` â€” API integration
- [ ] `PlatformAgent` â€” orchestrates multi-platform search and deduplication
- [ ] CLI commands: `connect <platform>`, `search`, `platforms`

### Phase 6 â€” MCP Integration (Days 12â€“13)
- [ ] FastMCP server with all tools registered (resume, ATS, job, platform, tracker)
- [ ] Test with Claude Code (`mcp.json` config)
- [ ] Test with Google Antigravity (`mcp_config.json`)
- [ ] Add confirmation tokens for write/export operations

### Phase 7 â€” Polish & Publish (Day 14+)
- [ ] Write a proper `README.md` with demo GIF
- [ ] Add `.env.example` with all required keys
- [ ] Add `storage/sessions/` to `.gitignore` (never commit session cookies)
- [ ] Write tests for all modules
- [ ] GitHub release with install instructions
- [ ] Optional: Gradio/Streamlit web UI for non-CLI users

---

## âš™ï¸ Configuration (.env)

> **MCP users (Claude Code / Antigravity): no LLM key needed.** The host AI handles all
> language generation. Only set an LLM key if you are using the standalone CLI.

```env
# â”€â”€ MCP MODE (Claude Code / Antigravity) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# No LLM key required. Leave LLM_PROVIDER unset or set to "none".
LLM_PROVIDER=none

# â”€â”€ CLI STANDALONE MODE â€” pick one â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Option 1: Anthropic (Claude) â€” best match for Claude Code users on CLI
# LLM_PROVIDER=anthropic
# ANTHROPIC_API_KEY=sk-ant-...
# ANTHROPIC_MODEL=claude-sonnet-4-5-20250929

# Option 2: Google Gemini â€” best match for Antigravity users on CLI
# LLM_PROVIDER=google
# GOOGLE_API_KEY=...
# GOOGLE_MODEL=gemini-2.0-flash

# Option 3: Ollama (free, local, no key needed)
# LLM_PROVIDER=ollama
# OLLAMA_BASE_URL=http://localhost:11434
# OLLAMA_MODEL=llama3

# â”€â”€ Paths â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
RESUME_PATH=assets/resume.pdf
OUTPUTS_DIR=outputs/
DB_PATH=storage/applications.db

# â”€â”€ ATS Settings â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ATS_KEYWORD_DENSITY_MAX=0.05   # flag if keyword appears more than 5% of word count
ATS_EXPERIENCE_GAP_MONTHS=6    # flag employment gaps longer than this
```

---

## ğŸ”§ Runtime Modes

### Mode 1 â€” Claude Code (MCP)
No LLM key needed. Claude handles all language tasks. Playwright required for job platform login.

Add to `.mcp.json` in your workspace root:
```json
{
  "mcpServers": {
    "job-autopilot": {
      "type": "stdio",
      "command": "<path-to-repo>/.venv/bin/python",
      "args": ["<path-to-repo>/server.py"]
    }
  }
}
```

**What Claude Code handles:** resume tailoring, cover letter writing, ATS suggestion summaries.
**What the MCP server handles:** Docling parsing, ATS scoring engine, SQLite tracking, Playwright sessions, file export.

---

### Mode 2 â€” Google Antigravity (MCP)
No LLM key needed. Gemini handles all language tasks. **Playwright is optional** â€” Antigravity's integrated browser can handle job platform login directly; Playwright is only needed for background/automated searches.

Add to `%USERPROFILE%\.gemini\antigravity\mcp_config.json`:
```json
{
  "mcpServers": {
    "job-autopilot": {
      "type": "stdio",
      "command": "<path-to-repo>\\.venv\\Scripts\\python.exe",
      "args": ["<path-to-repo>\\server.py"]
    }
  }
}
```

**What Antigravity handles:** resume tailoring, cover letter writing, ATS summaries, job platform browsing (via built-in browser).
**What the MCP server handles:** Docling parsing, ATS scoring engine, SQLite tracking, file export.

---

### Mode 3 â€” Standalone CLI
Requires an LLM key. Set `LLM_PROVIDER` in `.env` to `anthropic`, `google`, or `ollama`.
Playwright required for job platform login.

```bash
job-autopilot load assets/resume.pdf
job-autopilot search "Python developer" --location Bangalore
job-autopilot check-ats --job-id 3
job-autopilot tailor --job-id 3
job-autopilot export --job-id 3
```

---

## ğŸŒŸ What Makes This Stand Out on GitHub

- **Zero API keys for MCP users** â€” works out of the box with Claude Code and Antigravity; no setup friction
- **Works with both Claude Code and Google Antigravity** â€” first-class support for both; Antigravity users don't even need Playwright
- **Resume-driven job search** â€” searches based on your actual skills, not just a keyword you type
- **Reference resume benchmarking** â€” compares your resume against real open-source high-quality resumes for the same role
- **Full user control** â€” shows a diff before saving any resume changes; shows job card before submitting any application
- **Full ATS simulation** â€” 3-layer scoring engine: Docling for structure, LLM structured output for semantic keyword extraction, regex for contact/date validation. No traditional NLP library needed
- **Context-aware skill detection** â€” LLM infers skills from bullet text ("led the React rewrite" â†’ React), something spaCy or BERT models miss entirely
- **Docling-powered parsing** â€” detects multi-column layouts, headers/footers, reading order â€” things pdfplumber misses entirely
- **Multi-platform job search** â€” searches Naukri, LinkedIn, Indeed, CutShort and more in one command
- **Secure login** â€” one-time browser login; your password never touches the tool
- **India-first** â€” built around the platforms Indian job seekers actually use
- **CLI fallback** â€” works standalone too, with Anthropic, Google, or local Ollama

---

## ğŸ“¦ Dependencies

```toml
[project]
# Core â€” required for all modes. No NLP library needed.
dependencies = [
    "fastmcp",
    "typer",
    "httpx",
    "beautifulsoup4",
    "docling",          # IBM Research document parser (structure extraction)
    "python-docx",      # DOCX export only
    "sqlalchemy",
    "python-dotenv",
    "rich",             # pretty CLI output
]

[project.optional-dependencies]
# Install for standalone CLI with Claude / Anthropic API
anthropic = ["anthropic"]

# Install for standalone CLI with Google Gemini API
google = ["google-generativeai"]

# Install for standalone CLI with local models (free, no key, privacy-first)
ollama = ["ollama"]

# Install for job platform login (not needed for Antigravity users)
browser = ["playwright"]

# Install everything
all = ["anthropic", "google-generativeai", "ollama", "playwright"]
```

> **After install:**
> ```bash
> # Only if using job platform search without Antigravity:
> playwright install chromium
> ```
> No model downloads. No NLP library setup. That's it.

**Quick install guides:**

| You're using | Install command | LLM key needed? |
|---|---|---|
| Claude Code (MCP) | `pip install -e .` | âŒ None |
| Antigravity (MCP) | `pip install -e .` | âŒ None |
| CLI + Claude/Anthropic | `pip install -e ".[anthropic,browser]"` | âœ… Anthropic |
| CLI + Google/Gemini | `pip install -e ".[google,browser]"` | âœ… Google |
| CLI + local Ollama | `pip install -e ".[ollama,browser]"` | âŒ None (local) |

---

## ğŸ“š Key References

- [Docling GitHub](https://github.com/docling-project/docling) â€” IBM Research document parser
- [Docling Docs](https://docling-project.github.io/docling/) â€” Full API documentation
- [FastMCP](https://github.com/jlowin/fastmcp) â€” MCP server framework
- [Anthropic Structured Output](https://docs.anthropic.com/en/docs/build-with-claude/structured-outputs) â€” JSON mode for skill extraction
- [Ollama](https://ollama.com/) â€” Local LLM runtime for privacy-first CLI users

---

*Built by Mrityunjay Â· Inspired by the frustration of sending 100 resumes into the void*
